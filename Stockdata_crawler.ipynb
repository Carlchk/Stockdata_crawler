{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stockdata crawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1GLTHGLZXyUY2GNc__awoJ3hvwbUb55b4",
      "authorship_tag": "ABX9TyOaK3zrVlqIgrF/EavCNSUa"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KErdd-cild67"
      },
      "source": [
        "#Stockdata crawler (1mins)\n",
        "\n",
        "import random \n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os \n",
        "import datetime\n",
        "import requests  \n",
        "\n",
        "API_KEY_ARR = [\"WCW741MHOAGYHV7J\",\"9EOM89VBWKIGF0VU\",\"TXXTM13PFRNXBC8S\"]\n",
        "\n",
        "def downloadFile(ticker,path,yrIndex,monthIndex,file_url):\n",
        "  print(\"downloading {0}\".format(file_url))\n",
        "  temp_path = '{0}/{1}'.format(path,str(datetime.date.today()))\n",
        "  if not os.path.exists(temp_path):\n",
        "    os.makedirs(temp_path)  \n",
        "  r = requests.get(file_url, stream = True)\n",
        "  with open(\"{0}/{1}/yr{2}m{3}.csv\".format(path,str(datetime.date.today()),str(yrIndex),str(monthIndex)) , \"wb\") as file:  \n",
        "    for block in r.iter_content(): \n",
        "      if block:\n",
        "        file.write(block)\n",
        "\n",
        "def checkingResultFileExist(ticker): \n",
        "  root_path = '/content/drive/MyDrive/stockResearch'\n",
        "  dataset_path = root_path + '/dataset/{0}/result/{0}_result.csv'.format(ticker)\n",
        "  return os.path.exists(dataset_path)\n",
        "\n",
        "def fileExistHandler(ticker):\n",
        "  root_path = '/content/drive/MyDrive/stockResearch'\n",
        "  dataset_path = root_path + '/dataset/{0}'.format(ticker)\n",
        "  result_path = root_path + '/dataset/{0}/result/{0}_result.csv'.format(ticker)\n",
        "\n",
        "  tempdf = pd.read_csv(result_path, index_col=None, header=0)\n",
        "  tempdf = tempdf.sort_values(by='time',ascending=False)\n",
        "  lastDateOfData = tempdf['time'].values[0]\n",
        "  a = datetime.datetime.strptime(lastDateOfData, '%Y-%m-%d %H:%M:%S').date()\n",
        "  b = datetime.date.today()\n",
        "  for monthIndex in range(1,2 if ((b-a).days)//30 == 0 else (((b-a).days)//30)+1):\n",
        "    file_url = \"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol=\" + str(ticker) +\"&interval=1min&slice=year1month\"+ str(monthIndex) + \"&adjusted=true&apikey=\" + str(random.choice(API_KEY_ARR))\n",
        "    downloadFile(ticker,dataset_path,1,monthIndex,file_url)\n",
        "  tempPath = dataset_path+\"/\"+str(datetime.date.today())\n",
        "  all_files = glob.glob(tempPath + \"/*.csv\".format())\n",
        "  li = []\n",
        "  for filename in all_files:\n",
        "    df = pd.read_csv(filename, index_col=None, header=0)\n",
        "    li.append(df)\n",
        "  result = (pd.concat([tempdf, df]).drop_duplicates()).sort_values(by='time',ascending=False)\n",
        "  result.to_csv('{0}/result/{1}_result({2}).csv'.format(dataset_path,ticker,str(datetime.date.today()), index=False)) \n",
        "  print('Data Updated: {0}/result/{1}_result({2}).csv Created'.format(dataset_path,ticker,str(datetime.date.today())))\n",
        "\n",
        "def scrap(ticker):\n",
        "  if checkingResultFileExist(ticker):\n",
        "    fileExistHandler(ticker)\n",
        "  else:\n",
        "    root_path = '/content/drive/MyDrive/stockResearch'\n",
        "    dataset_path = root_path + '/dataset/{0}'.format(ticker)\n",
        "    tempPath = dataset_path+\"/\"+str(datetime.date.today())\n",
        "    if not os.path.exists(dataset_path):\n",
        "      os.makedirs(dataset_path)\n",
        "    for yrIndex in range(1,3):\n",
        "      for monthIndex in range(1,13):\n",
        "        file_url = \"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol=\" + str(ticker) +\"&interval=1min&slice=year\" + str(yrIndex) +\"month\"+ str(monthIndex) + \"&adjusted=true&apikey=\" + str(random.choice(API_KEY_ARR))\n",
        "        print(\"downloading {0}\".format(file_url))\n",
        "        downloadFile(ticker,dataset_path,yrIndex,monthIndex,file_url)\n",
        " \n",
        "    all_files = glob.glob(dataset_path + \"/*.csv\".format(ticker))\n",
        "    li = []\n",
        "    for filename in all_files:\n",
        "      df = pd.read_csv(filename, index_col=None, header=0)\n",
        "      li.append(df)\n",
        "    frame = pd.concat(li, axis=0, ignore_index=True)\n",
        "    frame = frame.sort_values(by='time',ascending=False)\n",
        "    if not os.path.exists(dataset_path+'/result'):\n",
        "      os.makedirs(dataset_path+'/result')\n",
        "    frame.to_csv('{0}/result/{1}_result.csv'.format(dataset_path,ticker), index=False) \n",
        "    print('Data Created: {0}/result/{1}_result.csv Created'.format(dataset_path,ticker))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3KS5ge_a347",
        "outputId": "644d25c8-48ea-4b3e-e3c5-5e0ffe6775eb"
      },
      "source": [
        "ticker = [\"AAPL\"]\n",
        "\n",
        "for tickerItem in ticker:\n",
        "  print(tickerItem)\n",
        "  scrap(tickerItem)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AAPL\n",
            "downloading https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY_EXTENDED&symbol=AAPL&interval=1min&slice=year1month1&adjusted=true&apikey=TXXTM13PFRNXBC8S\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}